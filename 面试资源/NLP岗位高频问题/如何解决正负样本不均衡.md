## 如何解决正负样本不均衡

### 1.1  什么是样本不均衡现象
  机器学习领域中样本不均衡现象随处可见。咱们举一些例子说明，图片分类任务中假如我们要做猫狗图片的识别任务，因为猫狗在日常生活中随处可见，所以对应的样本猫和狗的图片很好找，样本比较均衡，咱们能很容易的得到1W张猫的图片和1W张狗的图片。但是如果我们现在做狗和狼图片识别任务，那情况就有些变化了。我们还是能方便的得到1W张狗的图片，但是狼因为在生活中不怎么常见，所以在同样的数据采集成本下我们可能只得到100张甚至更少的狼的图片。

  还有比如CTR任务中我们需要预测用户是否会对广告进行点击，通常情况下曝光一个广告用户点击的比率非常低，这里我们假如给101个用户曝光广告可能只有一个人点击，那么得到的正负样本比例就为1:100。如果是更高层级的广告转化目标比如下载、付费等正负样本的比例就更低了。

  同样的例子会出现在文本分类任务中，假如我们要做一个识别是否对传奇游戏标签感兴趣的文本二分类器，用户搜索中这部分的比例非常少，也许1W条用户搜索query中只有50条甚至更少的样本属于正例。这种现象就是样本不均衡，因为样本会呈现一个长尾分布，头部的标签包含了大量的样本，而尾部的标签拥有很少的样本，就像下面这张图片中表现的那样出现一个长长的尾巴，所以这种现场也称为长尾现象。

  ![image](https://user-images.githubusercontent.com/59279781/120833610-90329d80-c594-11eb-809e-85f74678d4f8.png)

### 1.2  样本不均衡可能带来的问题

  上面讲了样本不均衡的现象在机器学习场景中经常出现，那么样本不均衡会带来什么问题呢？众所周知模型训练的本质是最小化损失函数，当某个类别的样本数量非常庞大，损失函数的值大部分被样本数量较大的类别所影响，导致的结果就是模型分类会倾向于样本量较大的类别。咱们拿上面文本分类的例子来说明，现在有1W条用户搜索的样本，其中50条和传奇游戏标签有关，9950条和传奇游戏标签无关，那么模型全部将样本预测为负例，也能得到99.5%的准确率，会让人有一种模型效果还不错的假象，但是实际的情况是这个模型根本没什么卵用，因为我们的目标是识别出这些数量较少的类别。这也是我们在实际业务场景中遇到的问题。

  总体来看，解决样本不均衡的问题主要从数据层面和模型层面来解决，下面会分别从理论到实践的角度分享样本不均衡问题的解决策略。


### 02 从数据层面解决样本不均衡问题
  现在我们遇到样本不均衡的问题，假如我们的正样本只有100条，而负样本可能有1W条。如果不采取任何策略，那么我们就是使用这1.01W条样本去训练模型。从数据层面解决样本不均衡的问题核心是通过人为控制正负样本的比例，分成欠采样和过采样两种。
  
#### 2.1 欠采样
  欠采样的基本做法是这样的，现在我们的正负样本比例为1:100。如果我们想让正负样本比例不超过1:10，那么模型训练的时候数量比较少的正样本也就是100条全部使用，而负样本随机挑选1000条，这样通过人为的方式我们把样本的正负比例强行控制在了1:10。这种方式存在一个问题，为了强行控制样本比例我们生生的舍去了那9000条负样本，这对于模型来说是莫大的损失。
  
  相比于简单的对负样本随机采样的欠采样方法，实际工作中我们会使用迭代预分类的方式来采样负样本。具体流程如下图所示：
    ![image](https://user-images.githubusercontent.com/59279781/120833753-bfe1a580-c594-11eb-9ca8-93276e89f5cf.png)
  
  首先我们会使用全部的正样本和从负例候选集中随机采样一部分负样本（这里假如是100条）去训练第一轮分类器；然后用第一轮分类器去预测负例候选集剩余的9900条数据，把9900条负例中预测为正例的样本（也就是预测错误的样本）再随机采样100条和第一轮训练的数据放到一起去训练第二轮分类器；同样的方法用第二轮分类器去预测负例候选集剩余的9800条数据，直到训练的第N轮分类器可以全部识别负例候选集，这就是使用迭代预分类的方式进行欠采样。

  相比于随机欠采样来说迭代预分类的欠采样方式能最大限度的利用负样本中差异性较大的负样本，从而在控制正负样本比例的基础上采样出了最有代表意义的负样本。
  
  欠采样的方式整体来说或多或少的会损失一些样本，对于那些需要控制样本量级的场景下比较合适。如果没有严格控制样本量级的要求那么下面的过采样可能会更加适合你。

#### 2.2 过采样
  过采样和上面的欠采样比较类似，都是人工干预控制样本的比例，不同的是过采样不会损失样本。还拿上面的例子，现在有正样本100条，负样本1W条，最简单的过采样方式是我们会使用全部的负样本1W条，但是为了维持正负样本比例，我们会从正样本中有放回的重复采样，直到获取了1000条正样本，也就是说有些正样本可能会被重复采样到，这样就能保持1:10的正负样本比例了。这是最简单的过采样方式。
  
### 03 从模型层面解决样本不均衡问题
  上面主要从数据的层面来解决样本不均衡的问题，本节主要从模型层面解决样本不均衡的问题。相比于控制正负样本的比例，我们还可以通过控制Loss损失函数来解决样本不均衡的问题。拿二分类任务来举例，通常使用交叉熵来计算损失，下面是交叉熵的公式：

  ![image](https://user-images.githubusercontent.com/59279781/120833939-f4edf800-c594-11eb-94d6-82d5cc69a8d3.png)
  基于类别加权的Loss其实就是添加了一个参数a，这个a主要用来控制正负样本对Loss带来不同的缩放效果，一般和样本数量成反比。还拿上面的例子举例，有100条正样本和1W条负样本，那么我们设置a的值为10000/10100，那么正样本对Loss的贡献值会乘以一个系数10000/10100，而负样本对Loss的贡献值则会乘以一个比较小的系数100/10100，这样相当于控制模型更加关注正样本对损失函数的影响。通过这种基于类别的加权的方式可以从不同类别的样本数量角度来控制Loss值，从而一定程度上解决了样本不均衡的问题。


#### 3.2 Focal Loss
  ![image](https://user-images.githubusercontent.com/59279781/120834132-341c4900-c595-11eb-84dd-be6a515803e0.png)


#### 3.3 GHM Loss
   Focal  Loss主要结合样本的难易区分程度来解决样本不均衡的问题，使得整个Loss的曲线平滑稳定的下降，但是对于一些特别难区分的样本比如离群点会存在问题。可能一个模型已经收敛训练的很好了，但是因为一些比如标注错误的离群点使得模型去关注这些样本，反而降低了模型的效果。比如下面的离群点图：
  ![image](https://user-images.githubusercontent.com/59279781/120834165-3ed6de00-c595-11eb-8394-7fc2799da716.png)
  
  ![image](https://user-images.githubusercontent.com/59279781/120834183-45655580-c595-11eb-9af6-8043b37dd459.png)

  ![image](https://user-images.githubusercontent.com/59279781/120834223-50b88100-c595-11eb-962e-42efd46111f2.png)
  
  ![image](https://user-images.githubusercontent.com/59279781/120834245-57df8f00-c595-11eb-8769-28f461a204fd.png)
  
  ![image](https://user-images.githubusercontent.com/59279781/120834272-5f9f3380-c595-11eb-8ef6-287aa6da5283.png)
 
###  其他解决样本不均衡问题的策略
  上面主要是从数据层面和模型损失函数来解决样本不均衡的问题，下面是一些其他的样本不均衡策略：

#### 4.1 调节阈值修改正负样本比例
  通常情况下Sigmoid函数会将大于0.5的阈值预测为正样本。这时候我们可以通过调节阈值来调整正负样本比例，比如设置0.3分作为阈值，将大于0.3的样本都判定为正样本，这样相当于增加了正样本的比例。
  
#### 4.2 利用半监督或自监督学习解决样本不均衡



  
