## 基础知识

###  【高频】梯度消失、爆炸的原因和解决方案，以及在RNN、LSTM中的解决情况
    循环神经网络模型的求解可以采用BPTT(Back Propagation Through Time，基于时间的反向传播)算法实现，BPTT实际上是反向传播算法的简单变种。
    如果将循环神经网络按时间展开成T层的前馈神经网络来理解，就和普通的反向传播算法没有什么区别了。循环神经网络的设计初衷之一就是能够捕获长距离输入之间的依赖。
    从结构上来看，循环神经网络也理应能够做到这一点。然而实践发现，使 用BPTT算法学习的循环神经网络并不能成功捕捉到长距离的依赖关系，这一现象 主要源于深度神经网络中的梯度消失。
    传统的循环神经网络梯度可以表示成连乘的形式。
    
    由于预测的误差是沿着神经网络的每一层反向传播的，因此当雅克比矩阵的 最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸;
    反之，若雅克比矩阵的最大特征值小于1，梯度的大小会呈指数缩小， 产生梯度消失。对于普通的前馈网络来说，梯度消失意味着无法通过加深网络层次来改善神经网络的预测效果，
    因为无论如何加深网络，只有靠近输出的若干层 才真正起到学习的作用。这使得循环神经网络模型很难学习到输入序列中的长距离依赖关系。
    
    梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值时，对梯度进行等比收缩。而梯度消失问题相对比较棘手，需要对模型本身进行改进。
    深度残差网络是对前馈神经网络的改进，通过残差学习的方式缓解了梯度 消失的现象，从而使得我们能够学习到更深层的网络表示;而对于循环神经网络来说，长短时记忆模型[23]
    及其变种门控循环单元(Gated recurrent unit，GRU)[24] 等模型通过加入门控机制，很大程度上弥补了梯度消失所带来的损失。
    
   ##### 具体解决方案
    (1) 方案1-预训练加微调
      此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，
      而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信
      念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方
      法有一定的好处，但是目前应用的不是很多了。

    (2)方案2-梯度剪切、正则
      梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。
    (3)方案3-relu、leakrelu、elu等激活函数
      Relu:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。
      relu的主要贡献在于:解决了梯度消失、爆炸的问题;计算方便，计算速度快;
      同时也存在一些缺点：由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决; 输出不是以0为中心的;
    (4)解决方案4-batchnorm
      Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过
      程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。


    (5)解决方案5-残差结构
    (6)解决方案6-LSTM
      LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)，LSTM通过它内部的“门”可
      以接下来更新的时候“记住”前几次训练的”残留记忆“，因此，经常用于生成文本中。


