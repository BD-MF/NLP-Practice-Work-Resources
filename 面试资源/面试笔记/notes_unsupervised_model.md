@[toc]
# 概况
无监督学习涵盖以下内容：
* 聚类算法
	* 硬聚类（每个样本只能属于1个类，也就是类和类之间的交集为空）
		* 层次聚类
		* k-means/k均值聚类
	* 软聚类（每个样本可以属于多个类，也就是类和类之间的交集不为空）
		* EM算法
* 降维
	* PCA
	* 潜在语义分析
* 概率模型估计
	* 概率潜在语义分析
	* 潜在狄利克雷分配
* 图分析
	* pagerank

# 常用的距离
## 闵可夫斯基距离
可见[监督模型笔记](https://blog.csdn.net/sinat_41679123/article/details/107144254)中的距离度量部分

## 马氏距离
## 相关系数
## 余弦距离
# 层次聚类

## 聚合/自下而上聚类
样本一开始是全部散装的一个一个为一类，然后根据距离计算再合并
## 分裂/自上而下聚类
样本一开始全部是一个类，然后根据距离分裂
# k-means（基于原型的聚类）
求得最优解是NP难问题
## 步骤
1. 随机选择k个样本点作为k个类的中心
2. 对剩下的n-k个样本，计算和每个类中心的距离，把样本划分到距离最近的那个类中
3. 更新每个类的中心（类中样本的平均值）
4. 重复2，3直到类的中心不再改变为止

## 注意点
**初始类的选择**
不同的初始样本点选择，可能会得到不同的聚类结果。
解决方案：
1. 多随机选几次初始样本点
2. 先用层次聚类聚到k个类停止，然后从聚好的k个类中随机选择1个样本点作为初始类中心

**k的选择**
实际使用时k一般是不知道的，可以采用肘部法则，绘制下面的图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200705234234400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzQxNjc5MTIz,size_16,color_FFFFFF,t_70)
横轴是k，纵轴是类的直径。从图片看k=3最合适。

图中的纵轴，可以用所有类的平均直径的方式来表征。

## 问题及改进
**对初始值的设置很敏感**
**k-means++**
背景原因：聚类中心们应该尽可能远
实现思想：初始确定聚类中心时，距离已有聚类中心更远的点更可能被选为下一个聚类中心
实现方法：先随机确定1个点，然后确定其他聚类中心时，每次都选和当前已经有的中心最远的点
**intelligent k-means、genetic k-means**

**噪声和离群值非常敏感**
**k-medians**
核心思想：类的中心不是平均数而是中位数
**k-medoids**

**只用于numerical类型数据，不适用于categorical类型数据**
**k-modes**
核心思想：距离计算用2个x之间不同的属性值个数来代表。例如`x1 = [1,2,1], x2 = [0,2,1]`，则`x1`和`x2`之间的距离是`1`

**不能解决非凸（non-convex）数据**
**kernel k-means**（类比svm的kernel）

# DBSCAN（密度聚类）
## 步骤　　
1. 从任一对象点p开始； 
2. 寻找并合并核心p对象直接密度可达（eps）的对象；
3. 如果p是一个核心点，则找到了一个聚类，如果p是一个边界点（即从p没有密度可达的点）则寻找下一个对象点；
4. 重复2、3，直到所有点都被处理。


